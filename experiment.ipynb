{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import requests\n",
    "from io import BytesIO, TextIOWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config / Environment\n",
    "# -----------------------\n",
    "BASE_URL = \"https://dumps.wikimedia.org/other/pageviews\"\n",
    "# local storage inside worker — choose a shared/ephemeral path depending on your infra\n",
    "BASE_WORKDIR = \"/tmp/wikipedia_pipeline\"\n",
    "RAW_DIR = os.path.join(BASE_WORKDIR, \"raw\")\n",
    "EXTRACTED_DIR = os.path.join(BASE_WORKDIR, \"extracted\")\n",
    "OUTPUT_DIR = os.path.join(BASE_WORKDIR, \"output\")\n",
    "\n",
    "# Companies to track (page titles exactly as in dumps)\n",
    "COMPANIES = [\"Apple_Inc.\", \"Microsoft\", \"Tesla,_Inc.\", \"Amazon.com\", \"Meta_Platforms\"]\n",
    "\n",
    "# Airflow connections/variables (set these in Airflow UI -> Admin -> Connections / Variables)\n",
    "POSTGRES_CONN_ID = \"postgres_core_sentiment\"  # must be configured in Airflow connections\n",
    "ALERT_EMAILS = [\"data-team@example.com\"]      # configure as you wish (list)\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in (RAW_DIR, EXTRACTED_DIR, OUTPUT_DIR):\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# -----------------------\n",
    "def _format_hour_filename(dt: datetime):\n",
    "    \"\"\"\n",
    "    Wikimedia file pattern: pageviews-{YYYY}{MM}{DD}-{HH}0000.gz\n",
    "    Example: pageviews-20251001-000000.gz for 2025-10-01 00:00:00 -> covers 23:00-00:00? (follow docs)\n",
    "    We'll use dt.hour formatted as two digits and append 0000.\n",
    "    \"\"\"\n",
    "    return f\"pageviews-{dt.strftime('%Y%m%d')}-{dt.strftime('%H')}0000.gz\"\n",
    "\n",
    "def _download_url(url: str) -> bytes:\n",
    "    \"\"\"Download content and return bytes. Raises exception if non-200.\"\"\"\n",
    "    resp = requests.get(url, stream=True, timeout=60)\n",
    "    if resp.status_code != 200:\n",
    "        raise AirflowFailException(f\"Failed to download {url} (status {resp.status_code})\")\n",
    "    return resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task implementations\n",
    "def download_data(ds, **context):\n",
    "    \"\"\"\n",
    "    Download pageviews file for the execution date/hour.\n",
    "    By default this DAG expects the logical execution time (ds) and an `execution_hour` variable in params\n",
    "    (or default to 00).\n",
    "    \"\"\"\n",
    "    # ds is YYYY-MM-DD ; optionally we allow the hour via dag_run.conf or Airflow params\n",
    "    dag_run = context.get(\"dag_run\")\n",
    "    conf = getattr(dag_run, \"conf\", {}) or {}\n",
    "    hour = conf.get(\"hour\", 0)  # integer 0-23; allow override at runtime\n",
    "    # Use ds + hour as the target datetime\n",
    "    dt = datetime.strptime(ds, \"%Y-%m-%d\").replace(hour=int(hour))\n",
    "    filename = _format_hour_filename(dt)\n",
    "    url = f\"{BASE_URL}/{dt.strftime('%Y')}/{dt.strftime('%Y')}-{dt.strftime('%m')}/{filename}\"\n",
    "    local_path = os.path.join(RAW_DIR, filename)\n",
    "    # If file already downloaded, skip (idempotency)\n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"[download_data] File already exists, skipping download: {local_path}\")\n",
    "        return local_path\n",
    "\n",
    "    print(f\"[download_data] Downloading {url}\")\n",
    "    try:\n",
    "        content = _download_url(url)\n",
    "    except Exception as e:\n",
    "        # bubble up so Airflow marks task failed and retries if configured\n",
    "        raise\n",
    "\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"[download_data] Saved to {local_path}\")\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(ds, **context):\n",
    "    \"\"\"\n",
    "    Read extracted text files and aggregate pageviews for COMPANIES.\n",
    "    Output CSV has columns: page, date_hour (YYYY-MM-DD HH:00:00), views\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    # Determine date_hour string from ds and conf hour to match download\n",
    "    dag_run = context.get(\"dag_run\")\n",
    "    conf = getattr(dag_run, \"conf\", {}) or {}\n",
    "    hour = int(conf.get(\"hour\", 0))\n",
    "    date_hour_dt = datetime.strptime(ds, \"%Y-%m-%d\").replace(hour=hour)\n",
    "    date_hour_str = date_hour_dt.strftime(\"%Y-%m-%d %H:00:00\")\n",
    "\n",
    "    for fname in os.listdir(EXTRACTED_DIR):\n",
    "        file_path = os.path.join(EXTRACTED_DIR, fname)\n",
    "        print(f\"[transform_data] Processing {file_path}\")\n",
    "        # file format: domain_code page_name view_count response_size\n",
    "        # whitespace separated — page_name may contain underscores but no spaces\n",
    "        try:\n",
    "            # Use pandas for speed but read with engine python to be robust to whitespace\n",
    "            df = pd.read_csv(file_path, sep=r\"\\s+\", header=None, names=[\"domain\", \"page\", \"views\", \"bytes\"], usecols=[0,1,2,3], engine=\"python\")\n",
    "        except Exception as e:\n",
    "            print(f\"[transform_data] Failed to parse {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df_filtered = df[df[\"page\"].isin(COMPANIES)]\n",
    "        if df_filtered.empty:\n",
    "            continue\n",
    "        df_agg = df_filtered.groupby(\"page\", as_index=False)[\"views\"].sum()\n",
    "        df_agg[\"date_hour\"] = date_hour_str\n",
    "        items.append(df_agg)\n",
    "\n",
    "    if not items:\n",
    "        # Nothing to store — raise or simply return. We'll just create an empty CSV to indicate run complete.\n",
    "        out_file = os.path.join(OUTPUT_DIR, f\"aggregated_{ds.replace('-', '')}_{hour:02d}.csv\")\n",
    "        pd.DataFrame(columns=[\"page\", \"views\", \"date_hour\"]).to_csv(out_file, index=False)\n",
    "        print(f\"[transform_data] No matching pageviews found. Created empty output: {out_file}\")\n",
    "        return out_file\n",
    "\n",
    "    final = pd.concat(items, ignore_index=True)\n",
    "    # If multiple extracted files exist for same page (rare in our pattern), sum them\n",
    "    final = final.groupby([\"page\",\"date_hour\"], as_index=False)[\"views\"].sum()\n",
    "\n",
    "    out_file = os.path.join(OUTPUT_DIR, f\"aggregated_{ds.replace('-', '')}_{hour:02d}.csv\")\n",
    "    final.to_csv(out_file, index=False)\n",
    "    print(f\"[transform_data] Aggregation complete. Wrote {out_file}\")\n",
    "    return out_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_postgres(ti, **context):\n",
    "    \"\"\"\n",
    "    Upsert aggregated CSV rows into Postgres.\n",
    "    Table schema expected:\n",
    "      CREATE TABLE IF NOT EXISTS pageviews_aggregated (\n",
    "        page TEXT NOT NULL,\n",
    "        date_hour TIMESTAMP NOT NULL,\n",
    "        views BIGINT NOT NULL,\n",
    "        PRIMARY KEY (page, date_hour)\n",
    "      );\n",
    "    Upsert uses ON CONFLICT (page, date_hour) DO UPDATE SET views = EXCLUDED.views\n",
    "    \"\"\"\n",
    "    csv_path = ti.xcom_pull(task_ids=\"transform_data\")\n",
    "    if not csv_path or not os.path.exists(csv_path):\n",
    "        raise AirflowFailException(f\"No aggregated CSV available at {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if df.empty:\n",
    "        print(\"[load_data_to_postgres] No rows to insert.\")\n",
    "        return\n",
    "\n",
    "    pg = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)\n",
    "    # Prepare rows as tuples\n",
    "    # Ensure date_hour castable to timestamp; PostgresHook will handle param substitution\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        # Normalize page and ensure types\n",
    "        page = str(r[\"page\"])\n",
    "        views = int(r[\"views\"])\n",
    "        date_hour = str(r[\"date_hour\"])  # 'YYYY-MM-DD HH:00:00'\n",
    "        rows.append((page, date_hour, views))\n",
    "\n",
    "    upsert_sql = \"\"\"\n",
    "    INSERT INTO pageviews_aggregated (page, date_hour, views)\n",
    "    VALUES (%s, %s, %s)\n",
    "    ON CONFLICT (page, date_hour)\n",
    "    DO UPDATE SET views = EXCLUDED.views;\n",
    "    \"\"\"\n",
    "\n",
    "    conn = pg.get_conn()\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.executemany(upsert_sql, rows)\n",
    "        conn.commit()\n",
    "        print(f\"[load_data_to_postgres] Upserted {len(rows)} rows into pageviews_aggregated\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b86aa247c6762e1192ef30d4a148d9cb2e1093f7e81dd7f812977548b8bc96e6"
  },
  "kernelspec": {
   "display_name": "Python 3.12.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
